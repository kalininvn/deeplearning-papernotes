## Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
[1609.08144](https://arxiv.org/abs/1609.08144)

### Введение
В статье описывается создание end-to-end системы машинного перевода: decoder-encoder сеть с 8 LSTM слоями, residual связями и механизмом attention. Приведены результаты экспериментов с пословной моделью с фиксированным словарем, посимвольной, смешанной и со специальным токенизатором на [wordpieces](https://github.com/google/sentencepiece). Для увеличения скорости вычислений сеть была квантизирована (вычисления с меньшей точностью). Для решения проблемы того, что cross-entropy loss не оптимизирует на самом деле BLUE метрику было добавлен элемент обучения с подкреплением.

Преимуществом нейросетевого машинного перевода (NMT) над phrased-based статистическим машинным переводом (PBSMT) является то, что он выучивает из текстов сам то, как надо переводить и не зависит от инженерных решений в SMT, таких как, например, выравнивание текстов, построение фразовых таблиц. На практике же NMT системы обычно показывают результаты хуже SMT. Есть три момента, которые могут это объяснить: NMT медленнее обучаются и генерируют перевод, плохо работают с редкими словами, иногда переводят не все предложение, "забывая" перевести какие-нибудь слова.

Авторы статьи предлагают архитектуру GNMT как решение трех обозначенных проблем:
* редкие слова - wordpieces
* долго обучать - распараллеленные на 8 GPU вычисления (по слою на GPU)
* долго генерится ответ / долго обучать - квантизация сети
* долго генерится ответ - wordpieces в том смысле, что если большой размер словаря, то долго считать softmax, а если посимвольная, то долго генерить по одному символу
* "забывание" что-то перевести - в лучевом поиске: coverage penalty (?) и нормализация длины (?)

### Архитектура модели

### 
